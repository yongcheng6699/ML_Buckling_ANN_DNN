{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "##  A 4-Hidden Layer Architecture\n",
    "What is happening here?\n",
    "    [1] A traditional ANN model.\n",
    "    [2] Import and preprocessing data. The dataset is split into training and testing sets. Then, a validation set allows monitoring the model's performance on unseen data during training, which helps in early stopping and preventing overfitting.\n",
    "    [3] Create a neural network model with specific hidden layer and the model uses the Sequential API from Keras, starting with an input layer, followed by a dense (fully connected) hidden layer with ReLU activation, and then an output layer with linear activation (for regression tasks).\n",
    "    [4] Model Training and Evaluation: run over a range of different number of neurons (from 20 to 80 with step of 10 to tune the model's hyperparameters.\n",
    "    For each configuration: The model is trained on the training data for 150 epochs, with validation data used for validation during training.\n",
    "    Performance metrics such as R-squared, RMSE (Root Mean Squared Error), and MAE (Mean Absolute Error) are computed on training, validation, and test sets.\n",
    "    [5] Plot loss vs the number of epochs & MAE vs the number of epochs\n",
    "    [6] Plot Correlation between the actual output and the predicted output with the training dataset\n",
    "    [7] Plot Correlation between the actual output and the predicted output with the testing dataset\n",
    "    [8] Lastly, SHAP (SHapley Additive exPlanations) values are computed to explain variable importance in the model using shap library. SHAP summary plots (shap.summary_plot) are created to show the impact of different features on the model predictions."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "254ef7c0cb13d7cf"
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd \n",
    "import numpy as np                                             \n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import r2_score\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "\n",
    "## Imports the data\n",
    "bucklingdata = pd.read_csv(\"CBL_SimulationResults.csv\", encoding='cp1252')\n",
    "input_data = bucklingdata.drop(['Critical Buckling Load (N)','Critical Buckling Load (kN)'], axis=1)\n",
    "output_data = bucklingdata['Critical Buckling Load (kN)']\n",
    "\n",
    "## Data splitting\n",
    "np.random.seed(0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_data, output_data,\n",
    "                                                   test_size=0.3,\n",
    "                                                    shuffle=False,\n",
    "                                                    stratify=None,\n",
    "                                                    random_state=42)\n",
    "\n",
    "# For validation number input and output data\n",
    "val_no = round(0.2*y_train.size)\n",
    "x_val = X_train[:val_no]\n",
    "y_val = y_train[:val_no]\n",
    "\n",
    "def create_model4H(hp_layer_1, hp_layer_2, hp_layer_3, hp_layer_4):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(11,)))\n",
    "    hp_activation = 'relu'\n",
    "    hp_learning_rate = 0.01\n",
    "    model.add(Dense(hp_layer_1, activation=hp_activation))\n",
    "    model.add(Dense(hp_layer_2, activation=hp_activation))\n",
    "    model.add(Dense(hp_layer_3, activation=hp_activation))\n",
    "    model.add(Dense(hp_layer_4, activation=hp_activation))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    model.compile(loss='mse',\n",
    "                  optimizer=Adam(learning_rate = hp_learning_rate),\n",
    "                  metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "# Specify the range for neurons in the first and second hidden layers\n",
    "min_neurons = 20\n",
    "max_neurons = 80\n",
    "step = 10\n",
    "\n",
    "neuron_configs = []\n",
    "\n",
    "# Generate configurations with increasing neurons in all four layers\n",
    "for neurons1 in range(min_neurons, max_neurons + 1, step):\n",
    "    for neurons2 in range(min_neurons, max_neurons + 1, step):\n",
    "        for neurons3 in range(min_neurons, max_neurons + 1, step):\n",
    "            for neurons4 in range(min_neurons, max_neurons + 1, step):\n",
    "                neuron_configs.append((neurons1, neurons2, neurons3, neurons4))\n",
    "                \n",
    "results = []\n",
    "\n",
    "## Train your model for a number of epochs, with the .fit()\n",
    "## Get the performance metrics for the network and the learning curves\n",
    "# Iterate over each configuration of neurons\n",
    "for i, (p1, p2, p3, p4) in enumerate(neuron_configs):\n",
    "    print(f\"Training model with configuration {i+1}: Hidden Layer 1={p1}, Hidden Layer 2={p2}, Hidden Layer 3={p3}, Hidden Layer 4={p4}\")\n",
    "    \n",
    "    # Create the model for the current configuration\n",
    "    model = create_model4H(p1, p2, p3, p4)\n",
    "    \n",
    "    # Fit the model\n",
    "    history = model.fit(X_train, y_train, epochs=150,\n",
    "                        validation_data=(x_val, y_val), batch_size=32,\n",
    "                        verbose=0)\n",
    "    \n",
    "    # Evaluate the model on training, validation, and test sets\n",
    "    trainmse, trainmae = model.evaluate(X_train, y_train, verbose=0)\n",
    "    valmse, valmae = model.evaluate(x_val, y_val, verbose=0)\n",
    "    testmse, testmae = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    # Predictions for R-squared calculation\n",
    "    train_pred = model.predict(X_train)\n",
    "    val_pred = model.predict(x_val)\n",
    "    test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate R-squared scores\n",
    "    train_r2 = r2_score(y_train, train_pred)\n",
    "    val_r2 = r2_score(y_val, val_pred)\n",
    "    test_r2 = r2_score(y_test, test_pred)\n",
    "    \n",
    "    # Store the evaluation metrics in a DataFrame\n",
    "    result_df = pd.DataFrame({\n",
    "        'Configuration': i + 1,\n",
    "        'Neurons_1HL': p1,\n",
    "        'Neurons_2HL': p2,\n",
    "        'Neurons_3HL': p3,\n",
    "        'Neurons_4HL': p4,\n",
    "        'Train R2': train_r2,\n",
    "        'Val R2': val_r2,\n",
    "        'Test R2': test_r2,\n",
    "        'Train RMSE': math.sqrt(trainmse),\n",
    "        'Val RMSE': math.sqrt(valmse),\n",
    "        'Test RMSE': math.sqrt(testmse),\n",
    "        'Train MAE': trainmae,\n",
    "        'Val MAE': valmae,\n",
    "        'Test MAE': testmae\n",
    "    }, index=[0])\n",
    "    \n",
    "    results.append(result_df)\n",
    "\n",
    "# Concatenate all results into a single DataFrame\n",
    "results_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "# Save the results to an Excel file\n",
    "results_df.to_excel('4HL_Neurons_Network_Results_TTS_V1.xlsx', index=False)\n",
    "print()\n",
    "print(\"------------Congrats! You have successfully save the results to 4HL_Neurons_Network_Results_TTS_V1------------\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ceaf0795223c07a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### loss vs the number of epochs & MAE vs the number of epochs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86294d0bb8230162"
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_loss(loss,val_loss):\n",
    "  plt.figure()\n",
    "  plt.style.use('bmh')\n",
    "  plt.grid(False)\n",
    "  plt.plot(loss, linewidth=2)\n",
    "  plt.plot(val_loss, linewidth=2)\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Val'], loc='upper right')\n",
    "  plt.show()\n",
    "\n",
    "def plot_mae(mae,val_mae):\n",
    "  plt.figure()\n",
    "  plt.style.use('ggplot')\n",
    "  plt.grid(False)\n",
    "  plt.plot(mae, linewidth=2)\n",
    "  plt.plot(val_mae, linewidth=2)\n",
    "  plt.ylabel('MAE')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Val'], loc='upper right')\n",
    "  plt.show()\n",
    "\n",
    "train_loss = (np.array(history.history[\"loss\"]))/100\n",
    "val_loss = np.array((history.history[\"val_loss\"]))/100\n",
    "\n",
    "plot_loss(train_loss, val_loss)\n",
    "plot_mae(history.history[\"mae\"], history.history[\"val_mae\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c91f49479e67a73f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Correlation between the actual output and the predicted output with the training dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e89025959a7df97"
  },
  {
   "cell_type": "code",
   "source": [
    "custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\n",
    "sns.set_theme(style=\"ticks\", rc=custom_params)\n",
    "a = plt.axes(aspect='equal')\n",
    "plt.scatter(y_train, train_pred, s = 20, alpha=0.7, c = 'brown')\n",
    "plt.xlabel('Actual Critical Buckling Loads (kN)')\n",
    "plt.ylabel('Predicted Critical Buckling Loads (kN) - Training set')\n",
    "plt.annotate(\"R-squared = {:.3f}\".format(r2_score(y_train, train_pred)), (5, 19))\n",
    "plt.xlim([4, 20])\n",
    "plt.ylim([4, 20])\n",
    "plt.plot([4, 20], [4, 20])\n",
    "plt.plot()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3fde11aebd540867",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Correlation between the actual output and the predicted output with the testing dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9dc1af46de6ae4e1"
  },
  {
   "cell_type": "code",
   "source": [
    "custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\n",
    "sns.set_theme(style=\"ticks\", rc=custom_params)\n",
    "a = plt.axes(aspect='equal')\n",
    "plt.scatter(y_test, test_pred, s = 20, alpha=0.7, c = 'brown')\n",
    "plt.xlabel('Actual Critical Buckling Loads (kN)')\n",
    "plt.ylabel('Predicted Critical Buckling Loads (kN) - Testing set')\n",
    "plt.annotate(\"R-squared = {:.3f}\".format(r2_score(y_test, test_pred)), (5, 13))\n",
    "plt.xlim([4, 14])\n",
    "plt.ylim([4, 14])\n",
    "plt.plot([4, 14], [4, 14])\n",
    "plt.plot()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1e4f9a51f2dbd54",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Variable importance SHAP plot"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "383138de3c060ea"
  },
  {
   "cell_type": "code",
   "source": [
    "### VARIABLE IMPORTANCE\n",
    "# See more here: https://shap-lrjball.readthedocs.io/en/latest/example_notebooks/plots/bar.html\n",
    "features = [\"Number of holes, nh\", \"Web-post width, WP (mm)\",  \"Web opening diameter, Do (mm)\",  \"Ply 1 (ø)\", \"Ply 2 (ø)\", \"Ply 3 (ø)\", \"Ply 4 (ø)\", \"Ply 5 (ø)\", \"Ply 6 (ø)\", \"Ply 7 (ø)\", \"Ply 8 (ø)\"]\n",
    "\n",
    "# Create a SHAP explainer using KernelExplainer\n",
    "explainer = shap.KernelExplainer(model, shap.sample(X_train, 50))\n",
    "\n",
    "# Calculate SHAP values for the test dataset\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Reshape shap_values to remove the last dimension\n",
    "shap_values_reshaped = shap_values.reshape(X_test.shape)  # Assuming X_test.shape is (389, 11)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62738fca91bc24a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualize feature importances using bar plot\n",
    "shap.summary_plot(shap_values_reshaped, X_test, feature_names=features, plot_type=\"bar\", plot_size=(9,4))\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64b3801b38e5f183",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Distribution density between the training and testing dataset at 70-30 split. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21f93ecd6dcee76"
  },
  {
   "cell_type": "code",
   "source": [
    "y_train_values = y_train.values.flatten()  # Convert to 1D array\n",
    "y_test_values = y_test.values.flatten()    # Convert to 1D array\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# Plot histogram for training data\n",
    "sns.histplot(y_train_values, kde=True, color='blue', label='Training Data')\n",
    "\n",
    "# Plot histogram for testing data\n",
    "sns.histplot(y_test_values, kde=True, color='red', label='Testing Data')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Critical Buckling Load (kN)')\n",
    "plt.ylabel('Density')\n",
    "#plt.title('Distribution of Critical Buckling Load between Training and Testing Data')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebce8e572e49987f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "705bd2dccfe87de5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
