{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "##  A 6-Hidden Layer Architecture\n",
    "What is happening here?\n",
    "    [1] A traditional ANN model.\n",
    "    [2] Import and preprocessing data. The dataset is split into training and testing sets. Then, a validation set allows monitoring the model's performance on unseen data during training, which helps in early stopping and preventing overfitting.\n",
    "    [3] Create a neural network model with specific hidden layer and the model uses the Sequential API from Keras, starting with an input layer, followed by a dense (fully connected) hidden layer with ReLU activation, and then an output layer with linear activation (for regression tasks).\n",
    "    [4] Model Training and Evaluation: run over a range of different number of neurons (from 20 to 80 with step of 20) to tune the model's hyperparameters.\n",
    "    For each configuration: The model is trained on the training data for 150 epochs, with validation data used for validation during training.\n",
    "    Performance metrics such as R-squared, RMSE (Root Mean Squared Error), and MAE (Mean Absolute Error) are computed on training, validation, and test sets.\n",
    "    [5] Plot loss vs the number of epochs & MAE vs the number of epochs\n",
    "    [6] Plot Correlation between the actual output and the predicted output with the training dataset\n",
    "    [7] Plot Correlation between the actual output and the predicted output with the testing dataset\n",
    "    [8] Lastly, SHAP (SHapley Additive exPlanations) values are computed to explain variable importance in the model using shap library. SHAP summary plots (shap.summary_plot) are created to show the impact of different features on the model predictions."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "254ef7c0cb13d7cf"
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np                                             \n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import r2_score\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "\n",
    "## Imports the data\n",
    "bucklingdata = pd.read_csv(\"CBL_SimulationResults.csv\", encoding='cp1252')\n",
    "input_data = bucklingdata.drop(['Critical Buckling Load (N)','Critical Buckling Load (kN)'], axis=1)\n",
    "output_data = bucklingdata['Critical Buckling Load (kN)']\n",
    "\n",
    "## Data splitting\n",
    "np.random.seed(0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_data, output_data,\n",
    "                                                   test_size=0.3,\n",
    "                                                    shuffle=False,\n",
    "                                                    stratify=None,\n",
    "                                                    random_state=42)\n",
    "\n",
    "# For validation number input and output data\n",
    "val_no = round(0.2*y_train.size)\n",
    "x_val = X_train[:val_no]\n",
    "y_val = y_train[:val_no]\n",
    "\n",
    "def create_model6H(hp_layer_1, hp_layer_2, hp_layer_3, hp_layer_4, hp_layer_5, hp_layer_6):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(11,)))\n",
    "    hp_activation = 'relu'\n",
    "    hp_learning_rate = 0.001\n",
    "    model.add(Dense(hp_layer_1, activation=hp_activation))\n",
    "    model.add(Dense(hp_layer_2, activation=hp_activation))\n",
    "    model.add(Dense(hp_layer_3, activation=hp_activation))\n",
    "    model.add(Dense(hp_layer_4, activation=hp_activation))\n",
    "    model.add(Dense(hp_layer_5, activation=hp_activation))\n",
    "    model.add(Dense(hp_layer_6, activation=hp_activation))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    # Compile your model with your optimizer, loss, and metrics\n",
    "    model.compile(loss='mse',\n",
    "                  optimizer=Adam(learning_rate = hp_learning_rate),\n",
    "                  metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "# Specify the range for neurons in the first and second hidden layers\n",
    "min_neurons = 20\n",
    "max_neurons = 80\n",
    "step = 20\n",
    "\n",
    "neuron_configs = []\n",
    "\n",
    "# Generate configurations with increasing neurons in all six layers\n",
    "for neurons1 in range(min_neurons, max_neurons + 1, step):\n",
    "    for neurons2 in range(min_neurons, max_neurons + 1, step):\n",
    "        for neurons3 in range(min_neurons, max_neurons + 1, step):\n",
    "            for neurons4 in range(min_neurons, max_neurons + 1, step):\n",
    "                for neurons5 in range(min_neurons, max_neurons + 1, step):\n",
    "                    for neurons6 in range(min_neurons, max_neurons + 1, step):\n",
    "                        neuron_configs.append((neurons1, neurons2, neurons3, neurons4, neurons5, neurons6))\n",
    "            \n",
    "results = []\n",
    "\n",
    "## Train your model for a number of epochs, with the .fit()\n",
    "## Get the performance metrics for the network and the learning curves\n",
    "# Iterate over each configuration of neurons\n",
    "for i, (p1, p2, p3, p4, p5, p6) in enumerate(neuron_configs):\n",
    "    print(f\"Training model with configuration {i+1}: Hidden Layer 1={p1}, Hidden Layer 2={p2}, Hidden Layer 3={p3}, Hidden Layer 4={p4}, Hidden Layer 5={p5},  Hidden Layer 6={p6}\")\n",
    "    \n",
    "    # Create the model for the current configuration\n",
    "    model = create_model6H(p1, p2, p3, p4, p5, p6)\n",
    "    \n",
    "    # Fit the model\n",
    "    history = model.fit(X_train, y_train, epochs=150,\n",
    "                        validation_data=(x_val, y_val), batch_size=32,\n",
    "                        verbose=0)\n",
    "    \n",
    "    # Evaluate the model on training, validation, and test sets\n",
    "    trainmse, trainmae = model.evaluate(X_train, y_train, verbose=0)\n",
    "    valmse, valmae = model.evaluate(x_val, y_val, verbose=0)\n",
    "    testmse, testmae = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    # Predictions for R-squared calculation\n",
    "    train_pred = model.predict(X_train)\n",
    "    val_pred = model.predict(x_val)\n",
    "    test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate R-squared scores\n",
    "    train_r2 = r2_score(y_train, train_pred)\n",
    "    val_r2 = r2_score(y_val, val_pred)\n",
    "    test_r2 = r2_score(y_test, test_pred)\n",
    "    \n",
    "    # Store the evaluation metrics in a DataFrame\n",
    "    result_df = pd.DataFrame({\n",
    "        'Configuration': i + 1,\n",
    "        'Neurons_1HL': p1,\n",
    "        'Neurons_2HL': p2,\n",
    "        'Neurons_3HL': p3,\n",
    "        'Neurons_4HL': p4,\n",
    "        'Neurons_5HL': p5,\n",
    "        'Neurons_6HL': p6,\n",
    "        'Train R2': train_r2,\n",
    "        'Val R2': val_r2,\n",
    "        'Test R2': test_r2,\n",
    "        'Train RMSE': math.sqrt(trainmse),\n",
    "        'Val RMSE': math.sqrt(valmse),\n",
    "        'Test RMSE': math.sqrt(testmse),\n",
    "        'Train MAE': trainmae,\n",
    "        'Val MAE': valmae,\n",
    "        'Test MAE': testmae\n",
    "    }, index=[0])\n",
    "    \n",
    "    results.append(result_df)\n",
    "\n",
    "# Concatenate all results into a single DataFrame\n",
    "results_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "# Save the results to an Excel file\n",
    "results_df.to_excel('6HL_Neurons_Network_Results_TTS.xlsx', index=False)\n",
    "print()\n",
    "print(\"------------Congrats! You have successfully save the results to 6HL_Neurons_Network_Results_TTS------------\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T10:38:07.384654Z",
     "start_time": "2024-04-18T10:38:04.283594Z"
    }
   },
   "id": "ceaf0795223c07a",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'pandas' has no attribute '_pandas_parser_CAPI' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m                                             \n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m train_test_split\n",
      "File \u001B[1;32m~\\PycharmProjects\\ML_Buckling_ANN_DNN\\venv\\Lib\\site-packages\\pandas\\__init__.py:151\u001B[0m\n\u001B[0;32m    132\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcomputation\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;28meval\u001B[39m\n\u001B[0;32m    134\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mreshape\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m    135\u001B[0m     concat,\n\u001B[0;32m    136\u001B[0m     lreshape,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    148\u001B[0m     qcut,\n\u001B[0;32m    149\u001B[0m )\n\u001B[1;32m--> 151\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m api, arrays, errors, io, plotting, tseries\n\u001B[0;32m    152\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m testing\n\u001B[0;32m    153\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_print_versions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m show_versions\n",
      "File \u001B[1;32m~\\PycharmProjects\\ML_Buckling_ANN_DNN\\venv\\Lib\\site-packages\\pandas\\api\\__init__.py:2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\" public toolkit API \"\"\"\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      3\u001B[0m     extensions,\n\u001B[0;32m      4\u001B[0m     indexers,\n\u001B[0;32m      5\u001B[0m     interchange,\n\u001B[0;32m      6\u001B[0m     types,\n\u001B[0;32m      7\u001B[0m     typing,\n\u001B[0;32m      8\u001B[0m )\n\u001B[0;32m     10\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minterchange\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mextensions\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtyping\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     16\u001B[0m ]\n",
      "File \u001B[1;32m~\\PycharmProjects\\ML_Buckling_ANN_DNN\\venv\\Lib\\site-packages\\pandas\\api\\typing\\__init__.py:31\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mwindow\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     20\u001B[0m     Expanding,\n\u001B[0;32m     21\u001B[0m     ExpandingGroupby,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     26\u001B[0m     Window,\n\u001B[0;32m     27\u001B[0m )\n\u001B[0;32m     29\u001B[0m \u001B[38;5;66;03m# TODO: Can't import Styler without importing jinja2\u001B[39;00m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;66;03m# from pandas.io.formats.style import Styler\u001B[39;00m\n\u001B[1;32m---> 31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mio\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjson\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_json\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m JsonReader\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mio\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m StataReader\n\u001B[0;32m     34\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m     35\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrameGroupBy\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     36\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatetimeIndexResamplerGroupby\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     54\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWindow\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     55\u001B[0m ]\n",
      "File \u001B[1;32m~\\PycharmProjects\\ML_Buckling_ANN_DNN\\venv\\Lib\\site-packages\\pandas\\io\\json\\__init__.py:1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mio\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjson\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_json\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      2\u001B[0m     read_json,\n\u001B[0;32m      3\u001B[0m     to_json,\n\u001B[0;32m      4\u001B[0m     ujson_dumps,\n\u001B[0;32m      5\u001B[0m     ujson_loads,\n\u001B[0;32m      6\u001B[0m )\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mio\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjson\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_table_schema\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m build_table_schema\n\u001B[0;32m      9\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mujson_dumps\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mujson_loads\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbuild_table_schema\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     15\u001B[0m ]\n",
      "File \u001B[1;32m~\\PycharmProjects\\ML_Buckling_ANN_DNN\\venv\\Lib\\site-packages\\pandas\\io\\json\\_json.py:71\u001B[0m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mio\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjson\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_normalize\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m convert_to_line_delimits\n\u001B[0;32m     67\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mio\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjson\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_table_schema\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     68\u001B[0m     build_table_schema,\n\u001B[0;32m     69\u001B[0m     parse_table_schema,\n\u001B[0;32m     70\u001B[0m )\n\u001B[1;32m---> 71\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mio\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mparsers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mreaders\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m validate_integer\n\u001B[0;32m     73\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m TYPE_CHECKING:\n\u001B[0;32m     74\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcollections\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mabc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     75\u001B[0m         Hashable,\n\u001B[0;32m     76\u001B[0m         Mapping,\n\u001B[0;32m     77\u001B[0m     )\n",
      "File \u001B[1;32m~\\PycharmProjects\\ML_Buckling_ANN_DNN\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\__init__.py:1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mio\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mparsers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mreaders\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      2\u001B[0m     TextFileReader,\n\u001B[0;32m      3\u001B[0m     TextParser,\n\u001B[0;32m      4\u001B[0m     read_csv,\n\u001B[0;32m      5\u001B[0m     read_fwf,\n\u001B[0;32m      6\u001B[0m     read_table,\n\u001B[0;32m      7\u001B[0m )\n\u001B[0;32m      9\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTextFileReader\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTextParser\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mread_csv\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mread_fwf\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mread_table\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\PycharmProjects\\ML_Buckling_ANN_DNN\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:32\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_config\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m using_copy_on_write\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_libs\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m lib\n\u001B[1;32m---> 32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_libs\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mparsers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m STR_NA_VALUES\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01merrors\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     34\u001B[0m     AbstractMethodError,\n\u001B[0;32m     35\u001B[0m     ParserWarning,\n\u001B[0;32m     36\u001B[0m )\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_decorators\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Appender\n",
      "File \u001B[1;32mparsers.pyx:1418\u001B[0m, in \u001B[0;36minit pandas._libs.parsers\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: partially initialized module 'pandas' has no attribute '_pandas_parser_CAPI' (most likely due to a circular import)"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "### loss vs the number of epochs & MAE vs the number of epochs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86294d0bb8230162"
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_loss(loss,val_loss):\n",
    "  plt.figure()\n",
    "  plt.style.use('bmh')\n",
    "  plt.grid(False)\n",
    "  plt.plot(loss, linewidth=2)\n",
    "  plt.plot(val_loss, linewidth=2)\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Val'], loc='upper right')\n",
    "  plt.show()\n",
    "\n",
    "def plot_mae(mae,val_mae):\n",
    "  plt.figure()\n",
    "  plt.style.use('ggplot')\n",
    "  plt.grid(False)\n",
    "  plt.plot(mae, linewidth=2)\n",
    "  plt.plot(val_mae, linewidth=2)\n",
    "  plt.ylabel('MAE')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Val'], loc='upper right')\n",
    "  plt.show()\n",
    "\n",
    "train_loss = (np.array(history.history[\"loss\"]))/100\n",
    "val_loss = np.array((history.history[\"val_loss\"]))/100\n",
    "\n",
    "plot_loss(train_loss, val_loss)\n",
    "plot_mae(history.history[\"mae\"], history.history[\"val_mae\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T08:21:34.491667Z",
     "start_time": "2024-04-18T08:21:34.491667Z"
    }
   },
   "id": "c91f49479e67a73f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Correlation between the actual output and the predicted output with the training dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e89025959a7df97"
  },
  {
   "cell_type": "code",
   "source": [
    "custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\n",
    "sns.set_theme(style=\"ticks\", rc=custom_params)\n",
    "a = plt.axes(aspect='equal')\n",
    "plt.scatter(y_train, train_pred, s = 20, alpha=0.7, c = 'brown')\n",
    "plt.xlabel('Actual Critical Buckling Loads (kN)')\n",
    "plt.ylabel('Predicted Critical Buckling Loads (kN)')\n",
    "plt.annotate(\"R-squared = {:.3f}\".format(r2_score(y_train, train_pred)), (5, 19))\n",
    "plt.xlim([4, 20])\n",
    "plt.ylim([4, 20])\n",
    "plt.plot([4, 20], [4, 20])\n",
    "plt.plot()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3fde11aebd540867",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Correlation between the actual output and the predicted output with the testing dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9dc1af46de6ae4e1"
  },
  {
   "cell_type": "code",
   "source": [
    "custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\n",
    "sns.set_theme(style=\"ticks\", rc=custom_params)\n",
    "a = plt.axes(aspect='equal')\n",
    "plt.scatter(y_test, test_pred, s = 20, alpha=0.7, c = 'brown')\n",
    "plt.xlabel('Actual Critical Buckling Loads (kN)')\n",
    "plt.ylabel('Predicted Critical Buckling Loads (kN)')\n",
    "plt.annotate(\"R-squared = {:.3f}\".format(r2_score(y_train, train_pred)), (5, 19))\n",
    "plt.xlim([4, 20])\n",
    "plt.ylim([4, 20])\n",
    "plt.plot([4, 20], [4, 20])\n",
    "plt.plot()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1e4f9a51f2dbd54",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Variable importance SHAP plot"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "383138de3c060ea"
  },
  {
   "cell_type": "code",
   "source": [
    "### VARIABLE IMPORTANCE\n",
    "# See more here: https://shap-lrjball.readthedocs.io/en/latest/example_notebooks/plots/bar.html\n",
    "features = [\"Number of holes, nh\", \"Web-post width, WP (mm)\",  \"Web opening diameter, Do (mm)\",  \"Ply 1 (ø)\", \"Ply 2 (ø)\", \"Ply 3 (ø)\", \"Ply 4 (ø)\", \"Ply 5 (ø)\", \"Ply 6 (ø)\", \"Ply 7 (ø)\", \"Ply 8 (ø)\"]\n",
    "\n",
    "# Create a SHAP explainer using KernelExplainer\n",
    "explainer = shap.KernelExplainer(model, shap.sample(X_train, 5))\n",
    "\n",
    "# Calculate SHAP values for the test dataset\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Reshape shap_values to remove the last dimension\n",
    "shap_values_reshaped = shap_values.reshape(X_test.shape)  # Assuming X_test.shape is (389, 11)\n",
    "\n",
    "# Visualize feature importances using bar plot\n",
    "shap.summary_plot(shap_values_reshaped, X_test, feature_names=features, plot_type=\"bar\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62738fca91bc24a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "64b3801b38e5f183",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
